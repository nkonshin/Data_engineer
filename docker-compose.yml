version: '3.8'
services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: data_engineer_backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./prompts:/app/prompts
      - ./data:/app/data
    environment:
      - LLM_ENABLED=1
      - OPENAI_BASE_URL=http://host.docker.internal:1234/v1
      - OPENAI_MODEL=qwen3-coder-30b-a3b-instruct-mlx
      - OPENAI_API_KEY=lm-studio
      - PG_CONN_STR=postgresql://data_engineer:password@postgres:5432/data_engineer_db
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_DB=default
      - HDFS_WEBHDFS=http://namenode:9870
      - HDFS_USER=root
    depends_on:
      - clickhouse
      - postgres
      - namenode
    networks:
      - denet
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 10s
      timeout: 5s
      retries: 5

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: data_engineer_frontend
    ports:
      - "3000:3000"
    stdin_open: true
    networks:
      - denet
    environment:
      - PORT=3000
      - HOST=0.0.0.0
      - CHOKIDAR_USEPOLLING=true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 10s
      timeout: 5s
      retries: 5

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    networks:
      - denet

  postgres:
    image: postgres:13
    container_name: postgres
    restart: always
    environment:
      POSTGRES_USER: data_engineer
      POSTGRES_PASSWORD: password
      POSTGRES_DB: data_engineer_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - denet

  # Single-node Hadoop HDFS (NameNode + DataNode)
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    platform: linux/amd64
    environment:
      - CLUSTER_NAME=local-hadoop
    ports:
      - "9870:9870"  # NameNode web UI
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop/hadoop.env
    networks:
      - denet

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    platform: linux/amd64
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    ports:
      - "9864:9864"  # DataNode web UI
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop/hadoop.env
    depends_on:
      - namenode
    networks:
      - denet

  airflow:
    image: apache/airflow:2.5.1
    container_name: airflow
    platform: linux/amd64
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False
    volumes:
      - ./backend/airflow_dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db init &&
               airflow scheduler &
               airflow webserver"
    networks:
      - denet

volumes:
  clickhouse_data:
  postgres_data:
  hdfs_namenode:
  hdfs_datanode:
networks:
  denet:
    driver: bridge


